# Building a Local Knowledge Graph and Persistent Codebase Memory

Developers can leverage a combination of code parsing tools, knowledge graph databases, and vector stores to create a local, continuously updated knowledge graph of a Python codebase. This system will capture modules, dependencies, and relationships, providing rich context to an LLM so you don’t have to repeat yourself about the codebase. Projects like Potpie already demonstrate the value of this approach – by **building a comprehensive knowledge graph of your code,** an AI agent can *“understand complex relationships and assist with everything from debugging to feature development”* ([GitHub - potpie-ai/potpie: Prompt-To-Agent : Create custom engineering agents for your codebase](https://github.com/potpie-ai/potpie#:~:text=building%20a%20comprehensive%20knowledge%20graph,from%20debugging%20to%20feature%20development)). Below, we outline recommended tools and methods for each component, along with a suggested architecture and workflow.

## Parsing and Understanding Python Code Structure

To construct a knowledge graph of the code, first parse the Python codebase to extract its structure (modules, classes, functions, and their relationships). Python offers built-in and third-party tools for this:

- **Python `ast` (Abstract Syntax Tree):** The built-in `ast` module can parse source files into a syntax tree of nodes (e.g. Module, Import, ClassDef, FunctionDef, Call). By walking the AST, you can discover definitions and references. For example, one can find all import statements to build a module dependency graph, or find all function calls within each function to build a call graph. Using `ast` avoids ad-hoc text parsing – *“you’re building a parser ... luckily, Python has a built-in parser `ast`... This will build a syntax tree of Python code and allow us to crawl it to find the function dependencies we’re looking for.”* ([ψML - Graph My Code 1: Creating a Graph of Function Dependencies in Python](https://simonstolarczyk.com/posts/graph/Graph_My_Code_1.html#:~:text=At%20some%20point%20in%20going,function%20dependencies%20we%E2%80%99re%20looking%20for)). AST-based analysis understands code structure (like where functions begin/end), which is far superior to naive text search (e.g. splitting code by lines or characters can break function definitions ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=Because%20of%20the%20splitter%20has,is%20divided%20into%20two%20chunks)) ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=Of%20course%2C%20there%20are%20also,one%20example%20taken%20from%20Wikipedia))). 

- **LibCST (Concrete Syntax Tree):** LibCST is a powerful alternative that parses Python code into a concrete syntax tree, preserving formatting and comments. It allows traversal and modification of code while retaining fidelity to the original source. This is useful if you plan to not only analyze but also rewrite or autopatch code. LibCST can handle multiple Python versions’ syntax and provides visitors to easily locate nodes of interest (functions, classes, imports, etc.) ([Instagram/LibCST: A concrete syntax tree parser and serializer ...](https://github.com/Instagram/LibCST#:~:text=Instagram%2FLibCST%3A%20A%20concrete%20syntax%20tree,comments%2C%20whitespaces%2C%20parentheses%2C%20etc)) ([libcst · PyPI](https://pypi.org/project/libcst/#:~:text=libcst%20%C2%B7%20PyPI%20LibCST%20parses,comments%2C%20whitespaces%2C%20parentheses%2C%20etc)). While LibCST operates at a slightly lower level (concrete syntax), it can be used similarly to AST for extracting structural information. In many cases, the simpler `ast` library is sufficient for building a knowledge graph (since formatting is usually not needed for structural analysis).

- **Static Analysis Libraries:** For more advanced use cases, consider libraries like **astroid** (used by Pylint), which builds an enhanced AST with convenient methods (e.g. to get a class’s base classes or navigate import references), or **Jedi**, which can statically infer references and help resolve identifiers to definitions in many cases. These can enrich the graph with resolved links (for example, mapping a method call to the actual function in the codebase if possible).

- **Call Graph Generators:** If your goal includes function call relationships, tools like **pyan** or **PyCG** can automate building a call graph. *“Pyan takes one or more Python source files, performs a (rather superficial) static analysis, and constructs a directed graph of the objects in the combined source, and how they define or use each other.”* ([GitHub - davidfraser/pyan: pyan is a Python module that performs static analysis of Python code to determine a call dependency graph between functions and methods. This is different from running the code and seeing which functions are called and how often; there are various tools that will generate a call graph in that way, usually using debugger or profiling trace hooks - for example: https://pycallgraph.readthedocs.org/  This code was originally written by Edmund Horner, and then modified by Juha Jeronen. See README for the original blog posts and links to their repositories.](https://github.com/davidfraser/pyan#:~:text=Pyan%20takes%20one%20or%20more,rendering%20by%20GraphViz%20or%20yEd)). This can identify which functions call which others (though keep in mind Python’s dynamic features may limit complete static analysis). You can integrate such a tool or implement similar logic by scanning AST `Call` nodes. For example, examine each `ast.Call` node; if the function called is a known name or attribute, link it to a function definition in the graph (possibly using heuristics or symbol tables gathered during AST traversal).

In summary, start by **parsing every Python module** in the repository. Use AST/LibCST to collect: modules and their imports, global variables, class definitions (with base classes), function definitions (with their parameters), and any function calls or attribute accesses inside function bodies. This will be the raw data to feed into the knowledge graph.

## Building a Local Knowledge Graph of the Codebase

With the structural information parsed, represent it as a graph: nodes for code entities and edges for relationships. Several frameworks can be used to store and query this graph locally:

- **Neo4j (Graph Database):** A popular choice for knowledge graphs, Neo4j is a high-performance graph database you can run locally or in a Docker container. It uses the Cypher query language to store and retrieve graph data. You can create a node type for each entity (e.g. **File**, **Class**, **Function**, **Variable**) and relationship types for connections (e.g. **IMPORTS** between modules, **DEFINED_IN** for functions in a file, **CALLS** for function call relationships, **INHERITS** for class inheritance, etc.). Neo4j excels at complex queries; for example, you could ask “find all functions in module X that eventually call function Y” as a graph traversal query. A recent example demonstrates storing a codebase graph in Neo4j and using it to answer questions – *“This graph can be stored in a graph database such as Neo4j, and our questions (“How many functions are defined in file X?”, “Which files use variable Y?”) can be answered with an LLM generating Cypher queries.”* ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=This%20graph%20can%20be%20stored,of%20generating%20the%20following%20answers)). In practice, you would use a Python driver (like `py2neo` or the official Neo4j Python driver) to insert nodes/edges after parsing the code, and to query them later. Neo4j’s ability to index properties and traverse relationships efficiently makes it suitable for large codebases. It also integrates well with LLM tools (discussed later) via Cypher-based QA chains.

- **NetworkX (Python Graph Library):** If you prefer to avoid running a database server, you can use **NetworkX**, a pure Python library for graph data. *“NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.”* ([NetworkX — NetworkX documentation](https://networkx.org/#:~:text=NetworkX%20is%20a%20Python%20package,and%20functions%20of%20complex%20networks)). You can build an in-memory graph where each node is, say, a string representing “module:Class:function” or a tuple identifying an entity, and edges as tuples linking them. NetworkX allows adding arbitrary attributes to nodes and edges (e.g. store a node type or code snippet as a property). While NetworkX can handle quite large graphs, querying it is done via Python (you’d write functions to traverse or filter nodes, or use NetworkX algorithms). This is fine for smaller projects or for prototyping. NetworkX also can export graphs to formats like GraphML or GEXF, which you could reload later for persistence. However, it’s not as query-optimized as a database and doesn’t have a query language – you’d be writing Python logic to answer questions. For straightforward needs (like visualising the import graph or doing simple reachability queries), it’s extremely handy and requires no setup aside from installing the library.

- **RDFLib (RDF triple store):** If you prefer a semantic web/ontology approach, Python’s **RDFLib** allows you to store the knowledge graph as RDF triples (subject-predicate-object). For instance, you could define triples like (`moduleA`, *imports*, `moduleB`), (`functionFoo`, *defined_in*, `moduleA`), etc., using an ontology for code relationships. RDFLib can persist data to a file or a database and supports SPARQL queries over the triples ([Querying with SPARQL — rdflib 7.1.4 documentation - Read the Docs](https://rdflib.readthedocs.io/en/stable/intro_to_sparql.html#:~:text=Querying%20with%20SPARQL%20%E2%80%94%20rdflib,a%20graph%20with%20the%20rdflib)) ([RdfLibraries - Python Wiki](https://wiki.python.org/moin/RdfLibraries#:~:text=RdfLibraries%20,%C2%B7%20SparqlWrapper%20is)). This could let you ask questions like “SELECT all functions where name = ‘load_data’ and defined_in some Module that imports ‘pandas’” in SPARQL. The benefit is a standardized schema and query language; the downside is added complexity (you’d need to define appropriate RDF classes/properties for code entities). RDFLib is fully local (no external server needed) and might be attractive if you want to integrate with other semantic data or use reasoners. For most code understanding tasks, a property graph (like Neo4j or NetworkX) is sufficient, but RDFLib is an option for those already comfortable with it.

- **Other Graph Solutions:** There are other graph databases and libraries as well. **Neo4j** was mentioned because it’s widely used (and Potpie uses it internally ([Leveraging Codebase Knowledge Graphs for Agentic Code Generation](https://potpie.ai/blog/knowledge-graphs-for-agentic-code-generation#:~:text=,natural%20language%20descriptions%20of%20functions))), but you could also consider **Memgraph** (another performant graph DB that has a free local version), or **TigerGraph** (though less convenient locally). If using a SQL database is easier for you, an alternative is to model code relationships in tables (for example, a table for imports, a table for function calls) and query with SQL – however, this sacrifices the flexibility of graph traversals. Given the need for complex relationship queries in code (which can be naturally recursive or graphy), a graph database or library is more suitable. 

**Representing the Codebase as a Graph:** However you store it, you’ll want to decide on the graph schema. A simple approach (inspired by recent research ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=Here%2C%20we%20can%20showcase%20how,will%20get%20the%20following%20graph))) is:

- Nodes:
  - **Module/File** nodes (representing each Python file or module).
  - **Class** nodes (with an edge to the Module node where they are defined).
  - **Function** nodes (edge to Module or Class node that defines them).
  - (Optional) **Variable/Attribute** nodes for global variables or class attributes.
  - (Optional) **AST nodes** for fine-grained representation (as in the example where each AST element is a node ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=use%20blue%20node%20to%20represent,will%20get%20the%20following%20graph))). This can be overkill, but you might include identifiers or expressions as needed.

- Edges:
  - **IMPORTS**: Module -> Module (if module A imports module B).
  - **DEFINED_IN** or **CONTAINS**: e.g. Module -> Class (module contains class), Module -> Function, Class -> Function (class contains method).
  - **CALLS**: Function -> Function (if a function calls another function; you might also have Function -> Method or Method -> Method).
  - **INHERITS**: Class -> Class (if class A inherits from class B).
  - **USES_VARIABLE** or **ASSIGNS**: for variable usage (if tracking variables).
  - **HAS_PARAMETER**: possibly link Function -> Variable for parameters, etc.
  - (Basically, any relationship you think would be useful to query can be an edge in the graph.)

For example, if you have `module1.py` that imports `module2.py`, and in `module1.py` a function `foo()` calls `module2.bar()`, your graph might have: nodes for *module1*, *module2*, *foo (function)*, *bar (function)*, and edges: *module1 IMPORTS module2*, *foo DEFINED_IN module1*, *bar DEFINED_IN module2*, and *foo CALLS bar*. A directory of files can also be a node (with **HAS_FILE** edges to files), if you want to model the folder hierarchy.

Design the schema according to what questions you want to answer. If you only care about module-level dependencies, a simpler graph of modules and IMPORTS edges suffices. If you want to trace how data flows or how functions relate, include function call edges. The Medium article *“Building Knowledge Graph over a Codebase for LLM”* shows an example graph where *“blue nodes represent file/directory, green nodes represent AST node”* with edges for file structure and AST parent-child relationships ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=use%20blue%20node%20to%20represent,will%20get%20the%20following%20graph)). That level of detail can answer questions like *“List all files where the variable `local_zone` is used”* by traversing identifier nodes ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=Generated%20Cypher%3A%20MATCH%20%28f%3Afile%29,%27filename%27%3A%20%27time_utils.py%27)), which is something a text search might miss if it lacks context.

To actually build the graph: after parsing each file’s AST, create nodes in your graph for the file, then for each class/function defined, and add appropriate edges (and likewise add edges for each import statement or function call found). This could be done in-memory first (e.g. build a NetworkX graph or just collect edge lists) and then loaded into Neo4j or another store in bulk.

## Semantic Representations of Code (Embeddings & Vector Stores)

Structural knowledge graphs excel at answering **structural or relational questions** (like “what calls what” or “where is this defined”), but they don’t capture the semantic meaning of code. For richer assistance (e.g. to recall what a function does or to fetch relevant code snippets by meaning), you’ll want to store **semantic embeddings** of the code. This essentially creates a **vector-indexed knowledge base** for the code, complementary to the graph.

**What to Embed:** You can embed any textual representation of code elements – for example, function or class docstrings, the source code of a function, or even a high-level summary of each function’s behavior. Potpie’s approach is to generate *“function-level explanations via a vector database for agents to query”* ([Leveraging Codebase Knowledge Graphs for Agentic Code Generation](https://potpie.ai/blog/knowledge-graphs-for-agentic-code-generation#:~:text=with%20better%20context%20by%20understanding,business%20logic%20across%20complex%20codebases)). In practice, one might iterate over all functions (or logical code blocks) and do the following for each:

- Get a representative text (source or summary). If the function has a docstring, that’s a great starting point (it often explains the function’s purpose). Otherwise, you might take the function’s source code (perhaps truncated or abstracted) or use an LLM to **generate a brief description** of the function. (You can prompt an LLM: “Summarise what this function does…” and use that result as the content to embed, storing the summary in your knowledge graph too.)
- Compute an **embedding vector** for that text using a model. Since we want a local solution, you’d use an open-source embedding model. Good choices include: **SentenceTransformers** models (like `all-MiniLM-L6-v2` or `multi-qa-mpnet-base` – not specifically trained on code but decent general semantic embeddings), or models specifically tuned for code like **CodeBERT**, **GraphCodeBERT**, or **CodeT5** (these can produce embeddings that capture code syntax and semantics). There are also instruct-tuned models like OpenAI’s text-embedding-ada-002 (if willing to use an API) or HuggingFace’s **InstructorXL** that can handle code reasonably. For simplicity, you might start with a SentenceTransformer model locally to embed code/comments into a high-dimensional vector.

- Store the vector along with metadata: e.g., an identifier of which function or file it came from, perhaps a link to the node in your knowledge graph, and maybe the text itself (or a summary). This allows you to later retrieve *relevant code pieces* by semantic similarity to a query.

**Vector Stores:** To manage these embeddings, use a local vector database. Two common choices are:

- **ChromaDB:** *“Chroma is an AI-native open-source vector database... it runs on your machine.”* ([Getting Started - Chroma Docs](https://docs.trychroma.com/getting-started#:~:text=Getting%20Started%20,and%20runs%20on%20your%20machine)) It’s a lightweight library you can pip install and use within your Python process (or as a separate server). Chroma allows creating collections of embeddings, adding new vectors with IDs and metadata, and querying by similarity (it uses approximate nearest neighbor search under the hood for speed). It also supports persistent storage (you can designate a directory for the database). Chroma is very user-friendly and integrates with LangChain out-of-the-box. For example, you could create a Chroma collection for “Functions” with embedding dim 384 (if using MiniLM) or 768, then upsert all function embeddings. At query time, you embed the user’s question and ask Chroma for the top-K closest code entries.

- **Weaviate:** Weaviate is a more heavyweight but feature-rich vector database, also open source. It can be run locally (via Docker, it requires a bit of memory) and provides a GraphQL interface for queries including vector search. One advantage is that Weaviate can also store a schema and non-vector data, and even mix symbolic filters with vector queries. However, for a single-developer local setup, Weaviate might be overkill unless you need its scaling or hybrid search capabilities. (If you have a very large codebase and need production-grade performance, Weaviate or **Qdrant** could be considered. Qdrant is another open-source vector DB that can run as a local service or embedded via a client library, offering fast cosine similarity search and filtering.)

- **FAISS or Annoy:** If you prefer not to run any database service or if your use-case is simpler, you can use libraries like **FAISS** (Facebook AI Similarity Search) or Spotify’s **Annoy** to store embeddings in memory and query them. These are essentially just indices; you’d have to manage metadata separately. For instance, FAISS can store millions of vectors and perform similarity search quickly. You could maintain a dictionary mapping index -> (function name, code reference) alongside the FAISS index. This is a lower-level approach; using Chroma or Qdrant etc. provides built-in metadata and persistence which is convenient.

Given the user’s needs (monolithic but modular codebase, frequent updates), an **embedded vector store like Chroma** is likely the easiest solution. It requires minimal setup and can be updated in real-time (you can add or delete vectors on the fly). Each embedding entry could include metadata such as `{"file": "module1.py", "function": "foo", "summary": "This function does X"}`.

**Use of Embeddings:** With the vector index in place, you gain a semantic search capability over your code. For example, the user could ask in natural language, “Where in the code do we format dates to `%Y-%m-%d`?” Your system can embed this query and retrieve nearest neighbors – which might include a function whose summary or code involves date formatting, even if the question didn’t use the exact function name. This helps catch context that a pure keyword or graph search might miss.

It’s worth noting that a **hybrid approach** often works best: use the knowledge graph to narrow down candidates (for instance, find relevant modules or components), then use embeddings to get the detailed content, or vice versa. In fact, advanced systems like Graphiti (discussed below) combine *“semantic embeddings, keyword (BM25), and graph-based search”* for efficient retrieval ([GitHub - getzep/graphiti at blog.getzep.com](https://github.com/getzep/graphiti?ref=blog.getzep.com#:~:text=%2A%20Bi,processing%2C%20suitable%20for%20enterprise%20environments)). For a starting point, you can use vector search to fetch relevant code snippets or summaries, and feed those to the LLM as additional context.

## Integration with LLMs for Persistent Context (Cursor AI and Others)

Once the knowledge graph and vector store are in place, the next challenge is **connecting this knowledge to your LLM** (e.g. the Cursor AI code assistant) so that it has persistent awareness of the codebase. There are a few integration patterns to consider:

- **Retrieval-Augmented Generation (RAG):** In this approach, you intercept the user’s query to the LLM (or the LLM’s internal need for information) and perform a retrieval from your knowledge stores, then prepend or append the results to the LLM prompt. For example, if you ask “Explain how data validation is implemented in this project,” a middleware could:
  1. Search the graph database for nodes related to “data validation” (perhaps a class named Validation or functions that have “validate” in the name).
  2. Use those to identify relevant code sections, then query the vector store for those sections or their summaries to get detailed descriptions.
  3. Construct a prompt to the LLM that includes these retrieved context pieces (e.g. “According to the codebase: ... [insert summaries/snippets] ... Now, answer the question.”).

  This can be done using frameworks like **LangChain** or **LlamaIndex**, which provide utilities to create a QA chain that first does a vector search and then feeds the results into the LLM’s context. LangChain, for instance, has a `VectorStoreRetriever` and even a `GraphCypherQAChain` that *“allows an LLM to provide a natural language interface to a graph database”* ([Neo4j - ️ LangChain](https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/#:~:text=Neo4j%20,Cypher%20is%20a)). In LangChain’s graph QA chain, the LLM is prompted to translate a user question into a Cypher query, which is run against Neo4j, and the results are then fed back into the LLM to form the answer ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=Generated%20Cypher%3A%20MATCH%20%28f%3Afile%20,NumberOfFunctions%27%3A%202)) ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=In%20this%20way%2C%20the%20LLM,chatbot%20is%20not%20capable%20answering)). This is a powerful method to answer questions like “Which module does X, and what functions does it call?” by letting the LLM dynamically query your knowledge graph.

- **Cursor’s Model Context Protocol (MCP):** Since you specifically use Cursor AI, it’s important to note how Cursor can be extended. Cursor (as of late 2024) introduced the Model Context Protocol which allows external “MCP servers” to supply tools or memory to the Cursor agent. In other words, you can run a local service that Cursor will query for additional context. A great example is the **Graphiti MCP server**. Graphiti (an open-source temporal knowledge graph framework by Zep) was used to add persistent memory to Cursor. The result was that *“the Cursor Agent can effectively retain, manage, and recall memory across developer and agent sessions”* ([Cursor IDE: Adding Memory With Graphiti MCP ⚡️](https://www.getzep.com/blog/cursor-adding-memory-with-graphiti-mcp/#:~:text=Cursor%2C%20the%20most%20popular%20agentic,have%20a%20good%20memory%20solution)). In this setup, Graphiti acts as the knowledge graph storage (with Neo4j under the hood for vectors), and Cursor calls it via MCP for any query that requires long-term knowledge. 

  You can mimic this by implementing your **own MCP server** that interfaces with your knowledge graph and vector store. Essentially, Cursor would pass your server a query (perhaps in natural language or a structured form), your server would perform searches (Cypher queries, vector lookups) on the local data, and return a result that the LLM can incorporate into the conversation. This requires some plumbing (following Cursor’s MCP documentation to register a custom server), but it achieves the goal of not having to stuff the entire codebase into Cursor’s limited context window. Instead, the agent fetches what it needs on demand.

  For example, if the agent (Cursor) is prompted with “Refactor the data processing pipeline,” your MCP server could be invoked to provide the definitions of relevant functions in that pipeline (from the graph relationships) and their purpose (from the vector store summaries). The Cursor AI then uses that info to guide its code generation. Without such memory, the agent might hallucinate or duplicate functionality. As one user noted, an IDE that *“can’t look at my entire context... I am not interested”* ([Anyone using Cursor AI and barely writing any code ... - Reddit](https://www.reddit.com/r/ChatGPTCoding/comments/1c1o8wm/anyone_using_cursor_ai_and_barely_writing_any/#:~:text=Anyone%20using%20Cursor%20AI%20and,2024%20I%20am%20not%20interested)) – so using MCP to give Cursor that context is the way forward.

- **Direct Query Tools for the LLM:** If not using Cursor’s protocol, another approach (for custom setups or other LLM interfaces) is to equip the LLM with tools. For instance, if using an open-source model via an API, you could implement a chat agent that has tools like “GraphQueryTool” and “VectorSearchTool.” The LLM, given a proper system prompt, could decide to call `GraphQueryTool("find class related to X")` which your code handles by hitting the Neo4j database and returning the result, then the LLM continues the conversation with that info. This is essentially how frameworks like LangChain operate with their tool-using agents. Potpie itself provides its agents with tools to query the code graph, such as an **`ask_knowledge_graph_queries`** tool that *“executes vector similarity searches to obtain relevant information”* ([GitHub - potpie-ai/potpie: Prompt-To-Agent : Create custom engineering agents for your codebase](https://github.com/potpie-ai/potpie#:~:text=,code%20graph%20structures%20for%20a)), and tools to fetch code by node ID or name ([GitHub - potpie-ai/potpie: Prompt-To-Agent : Create custom engineering agents for your codebase](https://github.com/potpie-ai/potpie#:~:text=,structures%20for%20a%20specific%20node)). This confirms the pattern: the LLM doesn’t hold all code in its head at once, but it can query the knowledge graph or vector index as needed.

- **Embedding the Knowledge in the Prompt (less dynamic):** A simpler but less scalable method is to pre-embed key knowledge into the LLM’s system prompt. For example, you might compile a high-level description of each module and include that in every session. However, this can get unwieldy as the project grows, and it doesn’t truly give “memory” (just a bigger prompt). It’s generally better to use one of the retrieval methods above so that context is fetched on demand and can scale to large codebases.

In summary, to integrate with Cursor AI for persistent context, **the MCP server approach is recommended** (since it’s designed for this purpose). You could either use Graphiti’s MCP (which might be more general, including temporal aspects you may not need), or create a custom one that on each request does: graph lookup + vector lookup -> returns answer or data. Cursor will treat that as an authoritative source of truth about your codebase.

If implementing outside of Cursor, frameworks like LangChain can simplify linking LLMs with Neo4j (via GraphCypherQAChain) and with vector stores (via VectorStoreRetriever). The combination can answer both structural queries and conceptual queries. This dual system helps the LLM avoid mistakes like redefining existing functions or using the wrong architecture – problems that arise without code context ([Leveraging Codebase Knowledge Graphs for Agentic Code Generation](https://potpie.ai/blog/knowledge-graphs-for-agentic-code-generation#:~:text=with%20better%20context%20by%20understanding,business%20logic%20across%20complex%20codebases)).

## Keeping the Knowledge Graph Up-to-Date (Real-Time Sync)

A key requirement is that the knowledge graph and memory stay in sync with your code, which is updated daily. There are several strategies to ensure **real-time or automated syncing** of the graph as the codebase evolves:

- **File Watchers:** Utilize a filesystem watcher (e.g. Python’s `watchdog` library) to monitor the project directory for changes (file edits, creations, deletions). When a `.py` file is modified, you can automatically trigger a re-parse of that file’s AST and update the relevant parts of the graph and vector index. For example, `watchdog` can call an event handler whenever a file changes ([Automating Code Change Tracking with Python and Watchdog | by Jordan | Medium](https://medium.com/@syncmail2024/automating-code-change-tracking-with-python-and-watchdog-389205089c34#:~:text=The%20Solution%3A)) ([Automating Code Change Tracking with Python and Watchdog | by Jordan | Medium](https://medium.com/@syncmail2024/automating-code-change-tracking-with-python-and-watchdog-389205089c34#:~:text=import%20time%20import%20logging%20from,events%20import%20FileSystemEventHandler%20import%20difflib)). In the handler, you could load the changed file’s AST, identify what changed (which functions were added/removed/modified), then update the knowledge graph: add new nodes/edges, remove or alter outdated ones. Similarly, update embeddings: recompute the embedding for any changed function or docstring and upsert it in the vector store (and delete embeddings for removed code). Using a watcher means the knowledge graph is near real-time – as soon as you save a file in your editor, the graph is updated within a second or two. This is great for interactive use with Cursor: the agent’s memory will always reflect the latest code.

- **Git Hooks or CI jobs:** Alternatively, you could update the knowledge graph at commit time or with a regular job. For instance, a Git pre-commit hook could run a script that regenerates parts of the graph for files in the commit. Or a nightly cron job could rebuild the whole index. This is simpler but means there’s a lag between coding and the memory update. If you’re using the system to assist your live coding (which seems to be the case), a file watcher approach provides a smoother experience.

- **Incremental Indexing:** Design your parsing script to handle updates incrementally. That is, you shouldn’t need to rebuild the entire graph from scratch on every change (especially in a large codebase, that could be slow). Instead, identify *what changed* and update only that. For example, if you changed one function’s logic but not its signature, you might not need to change the graph structure at all (unless you’re indexing something like which variables are used). You would just update the embedding for that function’s content in the vector store (and perhaps update any summary text stored). If you renamed a function or moved it, you would update node names and edges accordingly in the graph. An incremental approach can be complex to implement from scratch, but frameworks like Graphiti are built for this: *“Graphiti ... supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation”* ([GitHub - getzep/graphiti at blog.getzep.com](https://github.com/getzep/graphiti?ref=blog.getzep.com#:~:text=Graphiti%20is%20a%20framework%20for,aware%20AI%20applications)) ([GitHub - getzep/graphiti at blog.getzep.com](https://github.com/getzep/graphiti?ref=blog.getzep.com#:~:text=addresses%20these%20challenges%20by%20providing%3A)). If you use a simpler setup (like NetworkX or Neo4j with your own scripts), you’ll have to write some logic to do partial updates. One simple way: keep a mapping from file to the set of graph nodes that came from it. On file change, remove all those nodes/edges and re-add fresh ones from the new AST. This way you don’t accumulate stale data. Neo4j, for instance, can execute a transaction to delete all nodes with `file=XYZ` property and then insert new ones for that file.

- **Efficient Monitoring:** If using `watchdog`, be mindful of bouncing events (e.g. multiple events for one save). Debounce or batch them if necessary (e.g. if 10 files change due to a refactor, process them in one go). Logging the update actions helps to debug and ensure the graph stays consistent.

There are already prototypes for continuous code indexing. For example, **Code Indexer Loop** was a project that *“continuously and efficiently”* updated a vector index of code as it changed ([GitHub - definitive-io/code-indexer-loop: Code Indexer Loop is a Python library for indexing and retrieving source code files through an integrated vector database that's continuously and efficiently updated.](https://github.com/definitive-io/code-indexer-loop#:~:text=Code%20Indexer%20Loop%20is%20a,that%27s%20continuously%20and%20efficiently%20updated)). Similarly, Potpie presumably monitors the repo (they mention a tool for change_detection ([GitHub - potpie-ai/potpie: Prompt-To-Agent : Create custom engineering agents for your codebase](https://github.com/potpie-ai/potpie#:~:text=IDs%20simultaneously.%20,file%20structure%20of%20the%20codebase))). Drawing inspiration from these, you can achieve a near real-time update cycle.

**Real-time sync example:** Suppose you have the system running and you add a new function `def calculate_tax(...)` in `billing.py`. When you save the file, the watcher triggers:
  - Parse `billing.py`, identify the new function node.
  - Create a new Function node in the knowledge graph, with a **DEFINED_IN** edge to the `billing.py` file node, etc. If `calculate_tax` calls other functions or uses classes, add any new CALLS or REFERENCES edges.
  - Generate or update the summary for `calculate_tax` (maybe from its docstring) and embed it. Store the embedding in the vector DB with metadata (`file: billing.py`, `func: calculate_tax`).
  - Now the LLM agent can immediately answer questions about `calculate_tax` or include it in suggestions, even if the session started before the function existed.

One consideration: **persistent storage**. Ensure that if you stop and restart the tool, it reloads the latest state (especially if not using a persistent DB like Neo4j on disk or Chroma in persisted mode). For example, if using NetworkX, you might dump the graph to a file periodically (or at shutdown). With Neo4j or RDF stores, the data is on disk already. With Chroma, use `persist_directory` so it saves the index. This avoids a full re-index on every restart (though you might still implement a safety mechanism to rebuild if things desync).

## Suggested Architecture & Workflow

Bringing it all together, here’s a suggested architecture for implementing the system:

1. **Code Parsing Pipeline:** Set up a script or service that iterates over all files in the repository (e.g. all `.py` files not ignored) and parses them with `ast` or `LibCST`. This pipeline will extract:
   - Module-level info: imports, global definitions.
   - Class definitions (name, base classes, methods).
   - Function definitions (name, enclosing class or module, possibly identify which other functions they call or which variables they use).
   - Docstrings or top-of-function comments (to use for summarization).
   - (Optionally) other metadata like argument names and types (if using type hints, you could record those too).

   This pipeline can be run initially to build the whole knowledge graph from scratch. It should also be re-runnable for updates (either whole or per-file as discussed).

2. **Knowledge Graph Construction:** Initialize your chosen graph store (e.g. connect to a Neo4j instance or start building a NetworkX graph in memory). As you parse each element from step 1, create the corresponding nodes and relationships. It’s wise to give each node a unique identifier (like a combination of its fully qualified name and type). For example, you might identify a function by `billing.calculate_tax` (module + name) or use an internal ID. In a property graph, you can attach attributes like `name`, `type` (function/class/module), maybe `signature` for functions, etc., which can be helpful for queries. Insert edges for imports, containment (module->function), calls, etc., as described earlier.

   After this step, you have a graph of the entire codebase’s structure. You can already query it to answer things like “how many functions in module X” or “list all subclasses of class Y” directly with graph queries.

3. **Embedding Index Construction:** Next, build the semantic index. Loop through the functions (and perhaps classes or important modules) and prepare text for embedding. For each, you might use the docstring or generate a short summary (you could automate this with a GPT model if desired, one function at a time, or write a simple heuristic summary like “Function `{name}` (in `{module}`) – {first line of docstring or some key tokens}”). Compute embeddings using a local model. Then load these into a vector store:
   - If using **Chroma**, create a collection (e.g. `codebase_index`). As you add each document (function description), include metadata like `{"module": ..., "func": ..., "type": "function", "node_id": ...}` so you can identify it later.
   - If using **Neo4j 5+**, note that Neo4j now has vector indexing capabilities; Graphiti actually stores embeddings in Neo4j itself ([GitHub - getzep/graphiti at blog.getzep.com](https://github.com/getzep/graphiti?ref=blog.getzep.com#:~:text=Requirements%3A)). This means you could potentially keep everything in Neo4j – store an embedding as a property on a node and use Neo4j’s similarity search procedure. However, that’s advanced and not necessary; a separate Chroma or Weaviate might be simpler to manage.

   Verify the embedding step by trying a test query: take a sample question and ensure the nearest neighbor retrieval returns a relevant code snippet.

4. **Integration Layer (Agent or Middleware):** Implement the interface between the LLM (Cursor) and your knowledge base:
   - **Cursor MCP way:** Create an MCP server. This is typically a small web server that implements certain endpoints that Cursor will call. For instance, Cursor might send a request like `{"question": "What does function calculate_tax do?"}` to your server. Your server would parse that, perhaps recognize it’s asking about a function, then query the knowledge graph (find node for `calculate_tax`) and vector store (retrieve the summary or code for it). It would then respond with a summary or the actual code snippet. Cursor would inject that into the chat for the LLM to use. Essentially, your MCP server can decide how to handle different query types: you might even allow natural language queries and use an internal LLM to translate to graph queries, but since you have structured data, a few if/else or prompt parsing might suffice. (The specifics depend on MCP’s API, but the Cursor community forum and examples like Graphiti MCP ([Cursor IDE: Adding Memory With Graphiti MCP ⚡️](https://www.getzep.com/blog/cursor-adding-memory-with-graphiti-mcp/#:~:text=What%27s%20MCP%3F)) and Claude Memory MCP ([GitHub - apappascs/mcp-servers-hub: Discover the most comprehensive and up-to-date collection of MCP servers in the market. This repository serves as a centralized hub, offering an extensive catalog of open-source and proprietary MCP servers, complete with features, documentation links, and contributors.](https://github.com/apappascs/mcp-servers-hub#:~:text=Claude%20Memory%20Mcp%20%28%40whenmoon,11T02%3A29%3A39Z)) provide guidance).
   - **LangChain way (if not using Cursor’s interface):** Set up a LangChain agent with tools:
       - Tool 1: “CypherQuery” that given a text input uses an LLM to produce a Cypher query and executes it on Neo4j, returning results.
       - Tool 2: “VectorSearch” that given a question returns the top relevant code snippets from Chroma.
     Then wrap the OpenAI or other LLM with these tools enabled. A user question goes to the agent, it decides to call one or both tools, gets back data, and then it formulates an answer with that data. This is a bit complex to implement from scratch but LangChain provides chains like `GraphCypherQAChain` which can be used directly for graph QA ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=In%20this%20way%2C%20the%20LLM,chatbot%20is%20not%20capable%20answering)). For a simpler implementation, you could skip the agent and just do a two-step prompt: *First*, query the vector store yourself and get say 3 relevant texts; *then* feed a prompt to the LLM like: “Using the following context, answer the question... [context snippets here] ... Question: ...”.

   - **VS Code or Editor Integration:** (Since Cursor is itself an IDE, this might not apply, but for others reading) – If you were using VS Code, you might build an extension that on each user query, calls your local server for context and then calls an LLM API. Cursor’s design with MCP makes this easier for Cursor specifically.

5. **Real-Time Update Loop:** Run a background process (could be part of the MCP server or a separate daemon) to watch for file changes. On change, update the graph and embeddings as described. If using Neo4j, this means running a Cypher `MERGE` or `DELETE/CREATE` for the affected nodes/edges. For Chroma, use `collection.update` or re-add the embedding for the changed item. If performance is a concern, you might queue up changes and process one at a time to avoid overlap. This ensures that by the time you ask the assistant about new code, the info is there. If a large batch of changes happened, and you ask something immediately, you might implement a short delay or a manual “sync now” command to ensure consistency.

6. **Testing & Examples:** Finally, test the system with some use cases:
   - Ask the assistant a question that requires structural knowledge (“Which functions in module X call function Y?”) – this should trigger a graph query.
   - Ask a conceptual question (“How does the application validate user input?”) – this should retrieve relevant function summaries via the vector store.
   - Ask something requiring both (“Where is the output of function X used subsequently?”) – ideally the agent might do a graph traversal from function X to find callers, then use vector info to explain those contexts.
   - Ensure the answers make use of actual codebase info and not just general knowledge. If the LLM tends to hallucinate, you may need to tune prompts to strongly incorporate the retrieved data (e.g. *“Refer only to the following context…”*).

By combining these components, you end up with a **local codebase assistant** that has both the *graph-structured knowledge* of your code (for precise relationships and navigation) and the *semantic memory* of code content (for meaning and usage), all kept current with your latest edits. This effectively gives the LLM a form of long-term memory or context beyond its built-in window, enabling it to truly act as a smart pair programmer who “knows” your entire project.

## Conclusion

To recap, building a local knowledge graph for your Python codebase involves parsing the code with tools like AST/LibCST to extract structure, storing that structure in a graph (using Neo4j for robust querying or simpler tools like NetworkX for lightweight use), and augmenting it with semantic embeddings of the code using a vector store (such as Chroma for local persistence). With integration layers (like Cursor’s MCP or LangChain agents), you can equip an LLM with the ability to query this graph and memory on the fly, giving it persistent awareness of the codebase. The system should be designed to update continuously as code changes – using file watchers or hooks – so the LLM’s knowledge never falls out of sync. This architecture is akin to what Potpie does with its “built-in knowledge graph” for deep code understanding ([GitHub - potpie-ai/potpie: Prompt-To-Agent : Create custom engineering agents for your codebase](https://github.com/potpie-ai/potpie#:~:text=Why%20Potpie%3F)), but you have full control locally. 

By following this approach, your AI coding assistant will no longer be “flying blind” each session – it will have a grounded, up-to-date understanding of your entire Python project. This means more relevant answers, less repeated explanation from you, and a smoother development workflow.

**Sources:**

- Potpie AI (open-source codebase agent platform) – knowledge graph approach ([GitHub - potpie-ai/potpie: Prompt-To-Agent : Create custom engineering agents for your codebase](https://github.com/potpie-ai/potpie#:~:text=building%20a%20comprehensive%20knowledge%20graph,from%20debugging%20to%20feature%20development)) ([GitHub - potpie-ai/potpie: Prompt-To-Agent : Create custom engineering agents for your codebase](https://github.com/potpie-ai/potpie#:~:text=Why%20Potpie%3F))  
- Medium – *“Building Knowledge Graph over a Codebase for LLM”* – example schema and Neo4j QA ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=This%20graph%20can%20be%20stored,of%20generating%20the%20following%20answers)) ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=In%20this%20way%2C%20the%20LLM,chatbot%20is%20not%20capable%20answering))  
- Python AST parsing example (ψML blog) ([ψML - Graph My Code 1: Creating a Graph of Function Dependencies in Python](https://simonstolarczyk.com/posts/graph/Graph_My_Code_1.html#:~:text=At%20some%20point%20in%20going,function%20dependencies%20we%E2%80%99re%20looking%20for))  
- LangChain GraphCypherQAChain documentation ([Building Knowledge Graph over a Codebase for LLM | by Zimin Chen | Medium](https://medium.com/@ziche94/building-knowledge-graph-over-a-codebase-for-llm-245686917f96#:~:text=Generated%20Cypher%3A%20MATCH%20%28f%3Afile%20,NumberOfFunctions%27%3A%202))  
- Chroma vector DB documentation ([Getting Started - Chroma Docs](https://docs.trychroma.com/getting-started#:~:text=Getting%20Started%20,and%20runs%20on%20your%20machine))  
- Code Indexer Loop (continuous code embedding) ([GitHub - definitive-io/code-indexer-loop: Code Indexer Loop is a Python library for indexing and retrieving source code files through an integrated vector database that's continuously and efficiently updated.](https://github.com/definitive-io/code-indexer-loop#:~:text=Code%20Indexer%20Loop%20is%20a,that%27s%20continuously%20and%20efficiently%20updated))  
- Zep’s Graphiti framework (Cursor memory via MCP) ([Cursor IDE: Adding Memory With Graphiti MCP ⚡️](https://www.getzep.com/blog/cursor-adding-memory-with-graphiti-mcp/#:~:text=Cursor%2C%20the%20most%20popular%20agentic,have%20a%20good%20memory%20solution)) ([GitHub - getzep/graphiti at blog.getzep.com](https://github.com/getzep/graphiti?ref=blog.getzep.com#:~:text=addresses%20these%20challenges%20by%20providing%3A))  
- Additional citations inline for specific tools and points.
