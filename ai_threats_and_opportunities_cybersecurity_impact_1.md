# The Impact of AI on Cybersecurity: Opportunities and Threats

## Introduction

Artificial Intelligence (AI) is dramatically transforming the cybersecurity landscape. On one hand, cyber criminals are leveraging AI to launch more sophisticated attacks – from automated disinformation campaigns to deepfake scams – that can compromise businesses and societies. On the other hand, defenders are deploying AI to strengthen security, using intelligent tools to detect threats and fix vulnerabilities faster than ever. Global risk assessments reflect this dual impact: for example, the World Economic Forum ranks AI-driven disinformation among the top global risks for 2024 [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/]. In parallel, real incidents of AI-enabled cybercrime have surged. In 2024 alone, nearly 50 notable cases of deepfake-enabled fraud (impersonating people with AI) resulted in millions of dollars in losses [https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/]. This convergence of AI and cybersecurity presents both serious threats and promising opportunities.

This report provides a business-focused overview of how AI is affecting cybersecurity. We examine four key areas – disinformation, deepfake video/voice threats, AI-assisted code security, and the security of AI systems themselves – discussing the risks (“Threats”) and the defensive or beneficial uses (“Opportunities”) in each area, supported by recent real-world examples. The goal is to inform non-technical decision-makers about the evolving cyber risks posed by AI, as well as how AI can be harnessed to protect organizations.

---

## 1. Disinformation

### 1.1 Explanation

Disinformation refers to the deliberate creation and spread of false or misleading information, usually to influence opinions or obscure the truth. AI is supercharging disinformation by making it faster, cheaper, and easier to produce convincing fake content at scale. Generative AI models can instantly produce realistic-sounding text (fake news articles, social media posts, fake reviews) or images that appear authentic. Unlike in the past, propagandists no longer need large teams of writers or graphic artists – a single operator with a powerful AI tool can generate a flood of fabricated stories, complete with believable details and even AI-generated “photographs.” This advancement means malicious actors (whether financially motivated scammers, political agitators, or state-sponsored influence campaigns) can weaponize AI to manipulate public perception on a massive scale. The effects extend beyond social media rumors; AI-driven disinformation has the potential to sway elections, damage corporate reputations, and erode trust in institutions. In fact, experts warn that AI-enabled deepfakes and fake narratives could “fabricate convincing disinformation” that incites social discord or geopolitical tensions [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

For businesses, AI-fueled disinformation brings new risks. False information about a company’s product, a fabricated quote attributed to an executive, or a phony news report of a crisis can go viral before the truth can catch up. AI can mimic a company press release or a trusted journalist’s writing style, making lies harder to distinguish from legitimate news. The speed and scale of AI content generation also mean that a company facing a smear campaign or stock manipulation rumor might see thousands of posts and articles pop up overnight, overwhelming conventional PR responses. In short, AI has lowered the barrier for launching disinformation campaigns, creating a more chaotic information environment. Organizations must understand these threats and also explore AI-driven defenses to monitor and counter false narratives in real time.

### 1.2 Threats

- **1.2.1 Flood of AI-Generated False Content:**  
  Generative AI allows attackers to create “news” websites and social posts en masse with minimal effort. For example, a researcher demonstrated that widely available AI tools could run a fully automated disinformation operation for under $400 a month [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/]. This means propagandists can now flood social networks and inboxes with a deluge of fake articles and posts, outpacing manual content moderation. Analysts have already identified over 1,200 dubious websites across multiple languages that appear to be publishing news-like content almost entirely written by AI [https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/].

- **1.2.2 Hyper-Realistic Hoaxes and Images:**  
  AI-generated images and videos (often called “deepfakes” when they involve people) are being used to create fake scenes that can spark confusion or panic. One striking example occurred in May 2023, when an AI-generated image of an explosion at the Pentagon went viral on social media [https://www.reuters.com/article/fact-check/online-posts-reporting-explosion-near-pentagon-on-may-22-2023-are-false-idUSL1N37J2QJ/]. The fake photo – which analysts later noted had telltale AI-artifacts – was convincing enough to briefly cause market jitters before it was debunked. This incident highlights how AI hoaxes can threaten public safety and financial markets. A business could similarly be hurt by an AI-created photo or video falsely showing a crisis.

- **1.2.3 Erosion of Trust in Information:**  
  As AI-driven disinformation proliferates, overall trust in media and digital information deteriorates – a long-term strategic threat. When people know that realistic fakes are possible, they may start doubting everything, or conversely, some may believe sophisticated lies. This phenomenon, sometimes called the “liar’s dividend,” can be exploited by bad actors. For instance, if an official is caught in wrongdoing, they might dismiss authentic evidence as a deepfake. The U.S. Government Accountability Office notes that malicious deepfakes and disinformation could erode trust in elections and undermine national security [https://www.gao.gov/products/gao-24-107292].

### 1.3 Opportunities

- **1.3.1 AI-Powered Detection of Misinformation:**  
  Just as AI can create disinformation, it can help detect it. Advanced machine learning models can be trained to recognize patterns of AI-generated text or the subtle digital “fingerprints” of AI-edited images. For example, some detection algorithms analyze color or noise irregularities in images that often result from the generation process [https://www.gao.gov/products/gao-24-107292]. Automated fact-checking tools can cross-reference claims against reliable data sources to spot inconsistencies. Businesses can integrate such tools into their media monitoring systems, enabling faster response to disinformation.

- **1.3.2 Authentication and Content Provenance Technologies:**  
  Digital watermarking and content credentials are emerging as methods to verify authenticity. Images or videos produced by reputable sources could carry a cryptographic signature or metadata (invisible to normal viewing) that certifies their origin. If widely adopted, these systems can help social networks and news aggregators automatically check for authenticity markers, reducing the spread of AI-fabricated disinformation [https://www.gao.gov/products/gao-24-107292]. For businesses, embedding such provenance data in official communications can help audiences distinguish genuine messages from impersonations.

- **1.3.3 Rapid Counter-Messaging and Education:**  
  AI can be used constructively to counter disinformation once it’s identified. Organizations might deploy AI chatbots or content generators to quickly produce corrective information across multiple languages and tailored to various audiences. Such systems can be used to create fact-check articles, explanatory videos, or social media posts that counteract a viral fake narrative. This rapid response capability not only helps correct the public record but also serves as an educational tool, increasing overall media literacy and skepticism toward unchecked claims.

### 1.4 Real-World Examples

- **1.4.1 AI-Created Pentagon Hoax (2023):**  
  An AI-generated image of an explosion at the Pentagon was circulated on Twitter in May 2023, falsely implying a terror attack. The fake image, likely produced by a generative model, was widely shared and caused a brief dip in the stock market before officials debunked it. This case underscores how quickly AI-fueled disinformation can spread and the tangible impact it can have on markets and public trust [https://www.reuters.com/article/fact-check/online-posts-reporting-explosion-near-pentagon-on-may-22-2023-are-false-idUSL1N37J2QJ/].

- **1.4.2 AI-Generated “News” Websites:**  
  Analysts in 2023 and 2024 identified over 1,250 news websites operating with little to no human oversight, populated almost entirely by AI-written content [https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/]. These sites, often using generic names, were designed to attract clicks or propagate false narratives. Their proliferation demonstrates the scalability of AI disinformation and its potential to impact business reputations and market behavior.

- **1.4.3 Election Misinformation via Deepfakes (2024):**  
  During the 2024 election cycle, more than 100 deepfake video ads impersonating UK Prime Minister Rishi Sunak were identified on social media [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/]. These videos, which used AI-generated avatars and voice clones, aimed to mislead voters and stir public emotion. This overlap of disinformation and deepfake technology serves as a warning to businesses about the potential for AI to weaponize trusted faces and voices.

---

## 2. Deepfake Video and Voice Generation

### 2.1 Explanation

“Deepfakes” are hyper-realistic fake videos or audio recordings generated by AI. Using deep learning techniques, an AI can study footage or voice samples of a person and then produce new content where that person appears to say or do things they never did. With minimal input—such as a few minutes of speech or images—the technology can clone voices and generate lifelike videos. Initially popular in entertainment and social media for creating parodies or swapped faces, deepfakes have evolved to become incredibly convincing and accessible. Today, widely available apps can create passable deepfake videos with minimal expertise, challenging viewers’ ability to discern authenticity [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

The implications for cybersecurity and fraud are significant. In the past, video or audio evidence was often taken at face value. Deepfakes upend that trust by making it possible for criminals to impersonate an organization’s leader, public figure, or even a loved one through a fake video call or voicemail. While the technology has beneficial applications (e.g., in film production or language dubbing), its misuse for deception poses modern threats that businesses must prepare for.

### 2.2 Threats

- **2.2.1 Impersonation Fraud and Executive Scams:**  
  Deepfake technology enables highly believable impersonation, leading to new forms of CEO fraud. Criminals can present an employee with what appears to be direct orders from a company executive, using deepfake video or voice. In early 2024, a British company fell victim to a scheme where fraudsters used a deepfake video call of the CFO to convince an employee to transfer funds. Such scams, which blend social engineering with technological trickery, carry severe financial and reputational consequences.

- **2.2.2 Political Disinformation and Public Disorder:**  
  Deepfakes can be used to manufacture false statements or actions by public figures, intended to mislead the public or incite chaos. For example, audio deepfakes have been deployed to influence voters by impersonating political leaders with inflammatory messages. This misuse poses a national security threat by potentially suppressing voter turnout, inciting social unrest, or discrediting public officials [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

- **2.2.3 Harassment, Extortion, and Reputational Damage:**  
  Deepfakes are also used to create explicit or harmful content aimed at blackmail or damaging personal reputations. Non-consensual deepfake pornography, where a person’s face is superimposed onto adult content, has become a serious form of harassment. Additionally, a deepfake scandal—whether proven fake or not—can cause immediate reputational damage to individuals or organizations, leading to loss of trust and legal complications.

### 2.3 Opportunities

- **2.3.1 Deepfake Detection Technologies:**  
  As the threat grows, researchers are developing AI-powered detection tools that analyze videos and audio for subtle inconsistencies—such as irregular pixel patterns, unnatural blink rates, or misaligned lip-sync—that may reveal a deepfake. Integrating these tools into media monitoring systems can automatically flag suspect content before it spreads widely, thereby protecting brand reputation and public trust.

- **2.3.2 Authentication and Verification Measures:**  
  Implementing digital watermarking and secure authentication protocols can help verify the legitimacy of audio and video. Businesses can require that all official communications include cryptographic signatures or metadata that validate their origin. This layered approach to authentication makes it more difficult for attackers to successfully deploy deepfakes without detection.

- **2.3.3 Leverage for Training and Simulation:**  
  Deepfake technology can be harnessed for cybersecurity training. Companies can create simulated phishing calls or video messages that mimic deepfake attacks, enabling employees to learn how to identify suspicious cues and verify authenticity. Such simulations enhance awareness and build a culture of skepticism that can mitigate the impact of genuine deepfake attacks.

### 2.4 Real-World Examples

- **2.4.1 $243,000 Voice Scam (2019, with ongoing implications):**  
  Although the notable case of an energy firm in the UK falling victim to a $243,000 scam using AI voice cloning dates from 2019, similar deepfake voice scams have increased in frequency throughout 2023-2024, with numerous companies reporting attempts at voice-phishing. These incidents underscore the risk of deepfake-enabled impersonation in financial fraud [https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/].

- **2.4.2 Deepfake “Elon Musk” Scams (2024):**  
  In 2024, several deepfake videos emerged on social media, falsely depicting Elon Musk endorsing fraudulent cryptocurrency investments. Victims have reported significant financial losses—one individual lost $690,000—demonstrating how AI-generated impersonations of trusted figures can boost the credibility of scams [https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/].

- **2.4.3 Deepfake Political Attack Ads (2023):**  
  During election campaigns, deepfake technology has been employed to generate misleading political ads. In June 2023, a U.S. presidential campaign was rocked by an ad featuring AI-generated images of fictitious crises. Similarly, in Slovakia, deepfake audio was circulated to discredit a political candidate, underscoring the potential for deepfake technology to sow public disorder [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

---

## 3. Automatic Code Scanning and Vulnerability Remediation

### 3.1 Explanation

AI is revolutionizing the technical side of cybersecurity through automatic code scanning and vulnerability remediation. Automatic code scanning uses AI-driven tools to review source code and identify potential security vulnerabilities faster and more accurately than traditional methods. Vulnerability remediation refers to the process of fixing these identified issues. Modern AI systems can not only detect vulnerabilities—such as SQL injections or misconfigurations—but also propose or even apply fixes automatically.

These tools are powered by large language models trained on millions of lines of code. They can understand context, suggest fixes, and help developers address security flaws in near real-time. For instance, GitHub’s AI-powered tools have begun to auto-fix certain vulnerabilities, dramatically reducing the window of exposure and allowing development teams to focus on more complex challenges. While attackers are also using AI to discover vulnerabilities, the net effect for businesses can be positive when these technologies are used judiciously.

### 3.2 Threats

- **3.2.1 AI-Augmented Cyber Attacks on Code:**  
  Malicious actors are leveraging AI to automate the discovery and exploitation of vulnerabilities. Tools like “WormGPT” have emerged on underground forums, designed specifically to generate phishing emails, malware code, and other attack vectors with minimal input [https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html]. The ease of vulnerability discovery on the attacker’s side means businesses may face a barrage of automated attacks that outpace traditional security measures.

- **3.2.2 Incomplete or Insecure “Auto-Fixes”:**  
  Over-reliance on automated remediation tools can introduce new issues. Studies have found that a significant portion of AI-generated code suggestions may contain security weaknesses or only partially mitigate vulnerabilities. This can lead to a false sense of security if the fixes are accepted without proper human review [https://arxiv.org/abs/2310.02059]. Attackers might also learn to bypass standardized fixes, exploiting patterns in how the AI remediates vulnerabilities.

- **3.2.3 Privacy and Intellectual Property Risks:**  
  Many AI code scanning tools rely on cloud-based analysis, which may inadvertently expose sensitive or proprietary code. For instance, in 2023, Samsung experienced leaks when confidential source code was pasted into ChatGPT for debugging help [https://www.theregister.com/2023/04/06/samsung_reportedly_leaked_its_own/]. Such exposures can lead to data breaches, intellectual property violations, and regulatory compliance issues.

### 3.3 Opportunities

- **3.3.1 Faster and Scalable Vulnerability Detection:**  
  AI-powered scanners can comb through large codebases rapidly, identifying vulnerabilities that human reviewers might miss. Google’s *Security AI Workbench* and its Sec-PaLM model are examples of platforms that use AI to analyze code and malicious scripts in near real-time [https://virtualizationreview.com/articles/2023/04/24/google-ai-security.aspx]. For businesses, this means improved security with less manual effort.

- **3.3.2 Automated Vulnerability Remediation:**  
  Tools like GitHub’s *Code Scanning Autofix* can automatically suggest patches for common vulnerabilities. These systems provide not only a fix but also an explanation, which helps developers understand the security implications and reduces the time to patch. By reducing “security debt” and cutting remediation times, businesses can maintain more secure software deployments [https://www.bleepingcomputer.com/news/security/githubs-new-ai-powered-tool-auto-fixes-vulnerabilities-in-your-code/].

- **3.3.3 Empowering Developers and Security Teams:**  
  AI-driven security tools can serve as an ever-present assistant to developers, encouraging secure coding practices. By integrating AI into the software development lifecycle, organizations can “shift security left,” catching and resolving vulnerabilities during development rather than after deployment. This approach not only improves code quality but also frees human experts to focus on more complex security challenges.

### 3.4 Real-World Examples

- **3.4.1 GitHub Copilot Autofix (2024):**  
  In March 2024, GitHub launched its *Code Scanning Autofix* in public beta. This tool integrates with GitHub’s existing code scanning alerts and automatically generates patches for vulnerabilities in languages such as JavaScript, Python, and Java [https://www.bleepingcomputer.com/news/security/githubs-new-ai-powered-tool-auto-fixes-vulnerabilities-in-your-code/]. Early reports indicate that it can cover over 90% of common vulnerability types, significantly reducing remediation time.

- **3.4.2 Google’s Security AI Workbench (2023):**  
  At the RSA Conference in April 2023, Google introduced its *Security AI Workbench*, powered by the Sec-PaLM model. This platform is used in VirusTotal to analyze suspicious code, rapidly summarizing potential malicious behavior. It also helps scan open-source libraries for vulnerabilities, demonstrating AI’s role in proactively securing the software supply chain [https://virtualizationreview.com/articles/2023/04/24/google-ai-security.aspx].

- **3.4.3 WormGPT and Malicious Code Generation (2023):**  
  In mid-2023, cybercriminals began circulating WormGPT, an AI tool designed for generating phishing emails and malware code. Although its existence is alarming, it underscores the arms race in cybersecurity: as attackers harness AI, defenders must use similar technology to detect and mitigate these threats [https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html].

---

## 4. Testing and Securing LLMs

### 4.1 Explanation

As organizations deploy Large Language Models (LLMs) for various business applications, securing these AI systems themselves becomes a new cybersecurity frontier. LLMs, which power chatbots and automated content generators, are uniquely vulnerable because of their open-ended nature. Unlike traditional software with static code, LLMs generate responses dynamically, making them susceptible to novel threats such as prompt injection attacks—where a malicious prompt causes the AI to produce unintended, and potentially harmful, outputs.

Additionally, LLMs can inadvertently leak sensitive information if their training data is not properly managed or if user interactions are logged insecurely. As businesses integrate LLMs into customer service, decision support, and internal operations, ensuring these models do not become a liability is critical. Testing and securing LLMs involve proactive “red-teaming” (systematic testing for vulnerabilities) and implementing robust guardrails to limit the model’s actions and outputs.

### 4.2 Threats

- **4.2.1 Prompt Injection and Manipulation Attacks:**  
  Attackers can input malicious prompts that override an LLM’s built-in safety instructions. For example, a seemingly benign query might be crafted to cause the AI to reveal confidential internal data or execute an unintended command. Real-world incidents with Bing Chat in 2023 demonstrated that even sophisticated systems can be coaxed into exposing hidden instructions [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

- **4.2.2 Data Leakage and Privacy Breaches:**  
  LLMs often require vast amounts of training data, some of which might be sensitive. There is a risk that through either their outputs or logging practices, confidential information could be inadvertently exposed. The Samsung ChatGPT incident in 2023, where engineers leaked proprietary data by pasting it into a public AI tool, exemplifies this risk [https://www.theregister.com/2023/04/06/samsung_reportedly_leaked_its_own/].

- **4.2.3 Malicious Use and Model Exploits:**  
  Beyond manipulating outputs, LLMs themselves can be exploited as tools for malicious purposes. Attackers have, at times, managed to “jailbreak” models—circumventing built-in restrictions to obtain instructions for harmful actions. Additionally, adversaries may try to poison the training data, inserting subtle backdoors into the model that trigger harmful behavior under specific conditions.

### 4.3 Opportunities

- **4.3.1 Robust Testing and Red Teaming of AI Models:**  
  Proactively attacking AI models in a controlled setting helps identify vulnerabilities before adversaries can exploit them. Organizations like OpenAI have employed red teams to stress-test models such as GPT-4, gathering critical feedback to improve safeguards. Business leaders can adopt similar practices—either internally or by engaging third-party experts—to ensure their LLMs are resilient against prompt injection and other attacks [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

- **4.3.2 AI Guardrails and Monitoring Systems:**  
  Implementing multi-layered guardrails, such as prompt filtering, role-based instructions, and sandboxing, can prevent LLMs from executing dangerous commands. Continuous monitoring for anomalous behavior further ensures that if a model begins to deviate from expected behavior, administrators are alerted immediately. Additionally, watermarking AI outputs to track their origin is an emerging strategy that could help in auditing and accountability.

- **4.3.3 Secure Design and Access Controls for LLM Integration:**  
  Applying classic security principles—like the principle of least privilege, input sanitization, and two-factor confirmation for sensitive operations—can significantly reduce the risk of an LLM becoming a liability. For instance, restricting an AI’s access to only non-sensitive data and requiring human oversight for critical actions can mitigate many risks. By embedding these controls into the system architecture, companies can safely integrate LLMs into everyday operations while keeping potential misuse in check.

### 4.4 Real-World Examples

- **4.4.1 Bing Chat Prompt Injection (2023):**  
  Shortly after launch, users of Microsoft’s Bing AI chat discovered ways to manipulate the chatbot into revealing its hidden system instructions. These incidents—where attackers managed to bypass safety filters—led Microsoft to tighten its safeguards. This example underscores the importance of rigorous testing and robust prompt filtering when deploying LLMs [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

- **4.4.2 Samsung’s ChatGPT Data Leak (2023):**  
  In a series of incidents, Samsung employees inadvertently leaked sensitive semiconductor data into ChatGPT. This led to a swift internal ban on using external AI tools for sensitive work and spurred the development of a secure, internal AI system. Samsung’s experience highlights the risks of data leakage and the need for strict access controls when using LLMs [https://www.theregister.com/2023/04/06/samsung_reportedly_leaked_its_own/].

- **4.4.3 OpenAI Bug Bounty and Model Improvement (2023):**  
  OpenAI’s launch of a bug bounty program in 2023 encouraged external researchers to identify vulnerabilities in its models, leading to rapid fixes and improved safety features. This proactive, transparent approach to LLM security demonstrates an opportunity for businesses to engage with the broader community to continuously strengthen their AI systems [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/].

---

## Conclusion

AI’s impact on cybersecurity is a classic double-edged sword. On one side, threat actors are weaponizing AI to generate sophisticated disinformation, produce convincing deepfakes, and automate the discovery and exploitation of vulnerabilities. This new breed of cyberattacks can disrupt business operations, erode public trust, and create unforeseen challenges for traditional security measures. On the other side, AI offers powerful tools to detect, mitigate, and even preempt these threats. Automated code scanning, AI-driven content verification, and robust testing frameworks for LLMs can significantly strengthen an organization’s security posture.

For non-technical business leaders, the key takeaway is that while AI introduces novel and significant risks, it also provides unique opportunities to enhance cybersecurity. Investments in AI-powered security tools, continuous testing, and the integration of strict guardrails will be essential for staying ahead of adversaries. Ultimately, a balanced approach that leverages AI both as a tool for defense and as a subject of robust security measures will be critical in safeguarding organizational integrity in an increasingly AI-driven world.

---

## References

- [https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/]
- [https://www.weforum.org/stories/2024/02/4-ways-to-future-proof-against-deepfakes-in-2024-and-beyond/]
- [https://www.gao.gov/products/gao-24-107292]
- [https://arxiv.org/abs/2310.02059]
- [https://www.bleepingcomputer.com/news/security/githubs-new-ai-powered-tool-auto-fixes-vulnerabilities-in-your-code/]
- [https://virtualizationreview.com/articles/2023/04/24/google-ai-security.aspx]
- [https://www.reuters.com/article/fact-check/online-posts-reporting-explosion-near-pentagon-on-may-22-2023-are-false-idUSL1N37J2QJ/]
- [https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html]
- [https://www.theregister.com/2023/04/06/samsung_reportedly_leaked_its_own/]
